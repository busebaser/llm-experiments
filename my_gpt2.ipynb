{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e5930c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      4\u001b[0m GPT_CONFIG_124M \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m50257\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m }\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"n_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qvk_bias\" : False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b233342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90339719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qvk_bias=False):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_out = d_out\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = d_out // num_heads\n",
    "\n",
    "    self.W_query = nn.Linear(d_in, d_out, bias=qvk_bias)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=qvk_bias)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=qvk_bias)\n",
    "    self.out_proj = nn.Linear(d_out, d_out)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.register_buffer(\n",
    "        \"mask\",\n",
    "        torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
    "        )\n",
    "\n",
    "  def forward(self,x):\n",
    "    b,num_tokens,d_in = x.shape\n",
    "    keys = self.W_key(x)\n",
    "    queries = self.W_query(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "\n",
    "    keys = keys.transpose(1,2)\n",
    "    queries = queries.transpose(1,2)\n",
    "    values = values.transpose(1,2)\n",
    "\n",
    "    attention_scores = queries @ keys.transpose(2,3)\n",
    "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "    attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "    attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "    attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "    context_vec = (attention_weights @ values).transpose(1,2)\n",
    "\n",
    "    context_vec = context_vec.contiguous().view(b,num_tokens, self.d_out)\n",
    "    context_vec = self.out_proj(context_vec)\n",
    "\n",
    "    return context_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "  def __init__(self, emb_dim):\n",
    "    super().__init__()\n",
    "    self.eps = 1e-5\n",
    "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "  def forward(self,x):\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "    norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "    return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self,x):\n",
    "    return 0.5 * x * (1+ torch.tanh(\n",
    "        torch.sqrt(torch.tensor(2.0/ torch.pi)) * (x + 0.044715 * torch.pow(x,3))\n",
    "    ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "        GELU(),\n",
    "        nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self,cfg):\n",
    "    super().__init__()\n",
    "    self.attention = MultiHeadAttention(\n",
    "        d_in = cfg[\"emb_dim\"],\n",
    "        d_out = cfg[\"emb_dim\"],\n",
    "        context_length = cfg[\"context_length\"],\n",
    "        num_heads = cfg[\"n_heads\"],\n",
    "        dropout = cfg[\"drop_rate\"],\n",
    "        qvk_bias = cfg[\"qvk_bias\"]\n",
    "    )\n",
    "    self.ff = FeedForward(cfg)\n",
    "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "  def forward(self,x):\n",
    "    shortcut = x\n",
    "    x = self.norm1(x)\n",
    "    x = self.attention(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = x + shortcut\n",
    "\n",
    "    shortcut = x\n",
    "    x = self.norm2(x)\n",
    "    x = self.ff(x)\n",
    "    x = self.drop_shortcut(x)\n",
    "    x = x + shortcut\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "  def __init__(self, cfg):\n",
    "    super().__init__()\n",
    "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    self.trf_blocks = nn.Sequential(\n",
    "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "    self.out_head = nn.Linear(\n",
    "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "    )\n",
    "\n",
    "  def forward(self, in_idx):\n",
    "    batch_size, seq_len = in_idx.shape\n",
    "    tok_embeds = self.tok_emb(in_idx)\n",
    "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "    x = tok_embeds + pos_embeds\n",
    "    x = self.drop_emb(x)\n",
    "    x = self.trf_blocks(x)\n",
    "    x = self.final_norm(x)\n",
    "    logits = self.out_head(x)\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575131e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "  for _ in range(max_new_tokens):\n",
    "    idx_cond = idx[:, -context_size:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "      logits = model(idx_cond)\n",
    "\n",
    "    logits = logits[:,-1,:]\n",
    "\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "    idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "  return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1047f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\",  encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape: \", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db0804",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 6,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    "    )\n",
    "print(\"Output : \", out)\n",
    "print(\"Output length :\", len(out[0]))\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
